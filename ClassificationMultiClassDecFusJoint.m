% written by Soheil Bahrampour
% March 2015

% Script for task driven multimodal classification using l_{12} prior
% (joint sparsity)

%  Performing dictionary learning (in first phase unsupervised, later
%  supervised) on training data. Sparse codes generated are used as features
%  which will be feeded to multiclass quadratic
%  for classification. It should be straight forward to extend the code to cover
% other convex cost functions such as logistic regression. For more
% information see the relevant paper:

% Multimodal Task-Driven Dictionary Learning for Image Classification
% Soheil Bahrampour, Nasser M. Nasrabadi, Asok Ray, W. Kenneth Jenkins
% http://arxiv.org/abs/1502.01094

% please cite above paper if you use this code.

% The joint sparse coding is solved using ADMM algorithm. The algorithm is
% implemented in c to gain speed advantage and is linked hear using a mex
% file. Of course, one can use his own optimization algorithm instead of ADMM.
% The mex file is compiled for 64 system with a particulr articuture and it is not guarantted that it
% can be used efficiently with other systeme. 

function Atr=ClassificationMultiClassDecFusJoint(XArr,n,N,S)

%% set parameters for unsupervised and supervised dictionary learning algorithms
opts=[];
opts.lambda = 0.1; % regularization term for l_{12} norm
opts.lambda2 = 0; % Frobenius norm regularizer
opts.iterADMM = 1000; % Number of iterations for the ADMM algorithm to solve the joint sparse coding problem
opts.rho = 0.1; % inverse of the step size (lambda) for ADMM algorithm using the notation of the following paper: https://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf
opts.iterUnsupDic = 5; % number of iteratios over the training data set for unsupervised dictionary learning
opts.iterSupDic = 10; % number of iteratios over the training data set for supervised dictionary learning
opts.computeCost = 0; % flag to compute (1) or not compute (0) the cost of the dictionary learning. Use 1 initially to monitor convergence. It makes the code slower when it is 1.
opts.batchSize = 100; % batch size for unsupervised and supervised dictionary learning
opts.intercept = 0; % whether to add (1) or not add (1) intercept term to the classifier
ro = 5; % learning rate of the SGD of dictionary learning (sensitive parameter for convergence)
d = 100; % Number of dictionary atoms. For example, for a problem with 11 number of classes and 4 number of atoms per class, the dictionary size would be 4*11

%% set parameters for initial classifier training
nuQuad = 1e-8 % regularize for the classifier
iterQuad = 300; % number of iterations for training the classifier on the sparse codes generated by the unsupervised dictionary learning algorithm
batchSizeQuad = 100; % the batch size for training the classifier
roQuad = 20; % Learning rate for SGD algorithm to train the classifier
computeCostQuad = 0; % flag to compute (1) or not compute (0) the classification cost. Use 1 initially to monitor convergence. It makes the code slower when it is 1.


%% load and format multimodal training and test data
% S = 5; %number of modalities
 %n = [1024;1024];  % feature dimension for each modality. n has dimension (S*1).
 %N = 100; %Number of training samples
% XArr = Concatenated multimodal training samples: XArr = [X^1; X^2; ...;X^S] 
%   where X^s is matrix of n(s)*N dimension consiting of training samples
%   from the s-th modality
% YArr = Concatenated multimodal test data formed similar to XArr
% trls = vector of training lables having dimension of 1*N. Each entry
%   should be in range 1, ..., C, where C is the number of classes.
% ttls = vector of test data lables 
    
%load sampleMultimodalData % comment and load your own multimodal data as commented above!

%% unsupervised dictionary learning 

L = zeros(d*S, d);
U = zeros(d*S, d);
temp = 1
% unsupervised dictionary learning
DUnsup = OnlineUnsupTaskDrivDicLeaJointC(XArr, n, d, opts);
%DUnsup = nearestSPD(DUnsup1);

for s = 1:S
    [L((s-1)*d+1:s*d,:), U((s-1)*d+1:s*d,:)] = factor(DUnsup(temp:temp+n(s,1)-1,:), opts.rho);  % cash  L U factroziation for solving ADMM for whole batch
    temp = temp+n(s,1);
end
Atr = zeros(d*S, N); % A: Sparse Coefficients from training modalities that are concatinated. The sparse code is feeded into classifiers as feature vector.
for j = 1: N  % find A for each data separetely
    alpha = JointADMMEigenMex(DUnsup, XArr(:,j), n, opts.lambda, opts.rho, L, U, opts.iterADMM); % solve sparse coding problem
    Atr(:,j) = alpha(:);
end
% compute features for test samples


% format the labeled data for use in the quadratic cost function






